{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nhngan/anaconda3/envs/facial/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import hydra\n",
    "import pyrootutils\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from PIL import Image\n",
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations import Compose\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "from src.data.dlib_datamodule import TransformDataset  # noqa: E402\n",
    "from src import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import logging as log\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascPath = \"data/ibug_tiny/haarcascade_frontalface_default.xml\"  # for face detection\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "anterior = 0\n",
    "\n",
    "dog = cv2.imread('data/ibug_tiny/dog_filter.png')\n",
    "hat = cv2.imread('data/ibug_tiny/cowboy_hat.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_dog_filter(dog,fc,x,y,w,h):\n",
    "    face_width = w\n",
    "    face_height = h\n",
    "    \n",
    "    dog = cv2.resize(dog,(int(face_width*1.5),int(face_height*1.75)))\n",
    "    for i in range(int(face_height*1.75)):\n",
    "        for j in range(int(face_width*1.5)):\n",
    "            for k in range(3):\n",
    "                if dog[i][j][k]<235:\n",
    "                    fc[y+i-int(0.375*h)-1][x+j-int(0.25*w)][k] = dog[i][j][k]\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def put_hat(hat,fc,x,y,w,h):\n",
    "    \n",
    "    face_width = w\n",
    "    face_height = h\n",
    "    \n",
    "    hat_width = face_width+1\n",
    "    hat_height = int(0.35*face_height)+1\n",
    "    \n",
    "    hat = cv2.resize(hat,(hat_width,hat_height))\n",
    "    \n",
    "    for i in range(hat_height):\n",
    "        for j in range(hat_width):\n",
    "            for k in range(3):\n",
    "                if hat[i][j][k]<235:\n",
    "                    fc[y+i-int(0.25*face_height)][x+j][k] = hat[i][j][k]\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Draw a rectangle around the faces\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m (x, y, w, h) \u001b[39min\u001b[39;00m faces:\n\u001b[1;32m     21\u001b[0m     \u001b[39m#cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m#cv2.putText(frame,\"Person Detected\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     frame \u001b[39m=\u001b[39m put_hat(hat,frame,x,y,w,h)\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m anterior \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(faces):\n\u001b[1;32m     28\u001b[0m     anterior \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(faces)\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mput_hat\u001b[0;34m(hat, fc, x, y, w, h)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[1;32m     14\u001b[0m             \u001b[39mif\u001b[39;00m hat[i][j][k]\u001b[39m<\u001b[39m\u001b[39m235\u001b[39m:\n\u001b[0;32m---> 15\u001b[0m                 fc[y\u001b[39m+\u001b[39mi\u001b[39m-\u001b[39m\u001b[39mint\u001b[39m(\u001b[39m0.25\u001b[39m\u001b[39m*\u001b[39mface_height)][x\u001b[39m+\u001b[39;49mj][k] \u001b[39m=\u001b[39m hat[i][j][k]\n\u001b[1;32m     16\u001b[0m \u001b[39mreturn\u001b[39;00m fc\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    if not video_capture.isOpened():\n",
    "        print('Unable to load camera.')\n",
    "        sleep(5)\n",
    "        pass\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(40,40)\n",
    "    )\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        #cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        #cv2.putText(frame,\"Person Detected\",(x,y),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "        frame = put_hat(hat,frame,x,y,w,h)\n",
    "            \n",
    "            \n",
    "            \n",
    "    if anterior != len(faces):\n",
    "        anterior = len(faces)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Snapchat-Filter'...\n",
      "remote: Enumerating objects: 104, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
      "remote: Total 104 (delta 9), reused 14 (delta 7), pack-reused 83\u001b[K\n",
      "Receiving objects: 100% (104/104), 72.05 MiB | 1.02 MiB/s, done.\n",
      "Resolving deltas: 100% (40/40), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/oflynned/Snapchat-Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter\n",
      "Collecting dlib\n",
      "  Downloading dlib-19.24.1.tar.gz (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /home/nhngan/anaconda3/envs/lht/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (4.7.0.72)\n",
      "Requirement already satisfied: numpy in /home/nhngan/anaconda3/envs/lht/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.24.2)\n",
      "Building wheels for collected packages: dlib\n",
      "  Building wheel for dlib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dlib: filename=dlib-19.24.1-cp39-cp39-linux_x86_64.whl size=4017423 sha256=62bae8096e51a7cf7b0f78a01f89f62abb5f036771bd8d01c0e6fca055934bd3\n",
      "  Stored in directory: /home/nhngan/.cache/pip/wheels/b1/fa/fa/a698544859f4356753d13f9217204f35427408e8bf07c3f238\n",
      "Successfully built dlib\n",
      "Installing collected packages: dlib\n",
      "Successfully installed dlib-19.24.1\n",
      "--2023-04-22 00:40:40--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
      "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
      "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64040097 (61M)\n",
      "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
      "\n",
      "shape_predictor_68_ 100%[===================>]  61.07M   128KB/s    in 13m 1s  \n",
      "\n",
      "2023-04-22 00:53:42 (80.1 KB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
      "\n",
      "/home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter/resources\n",
      "bunzip2: Can't open input file /content/Snapchat-Filter/shape_predictor_68_face_landmarks.dat.bz2: No such file or directory.\n",
      "mv: cannot stat '/content/Snapchat-Filter/shape_predictor_68_face_landmarks.dat': No such file or directory\n",
      "/home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter\n"
     ]
    }
   ],
   "source": [
    "%cd Snapchat-Filter\n",
    "!pip3 install -r requirements.txt\n",
    "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "%mkdir -p resources;\n",
    "%cd resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move '/home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter/shape_predictor_68_face_landmarks.dat' to '/content/Snapchat-Filter/resources/': No such file or directory\n",
      "/home/nhngan/Desktop/Projects\n"
     ]
    }
   ],
   "source": [
    "!bunzip2 /home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter/shape_predictor_68_face_landmarks.dat.bz2\n",
    "!mv /home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter/shape_predictor_68_face_landmarks.dat /home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter/resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter\n"
     ]
    }
   ],
   "source": [
    "%cd /home/nhngan/Desktop/Projects/facial_landmarks-wandb/Snapchat-Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "!python main.py --filter glasses --footage glasses.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "from video import create_capture\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_path = \"resources/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_dlib_pybind11.fhog_object_detector'>\n",
      "<class '_dlib_pybind11.shape_predictor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(detector))\n",
    "print(type(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR = 1\n",
    "FEATHER_AMOUNT = 11\n",
    "COLOUR_CORRECT_BLUR = 0.5\n",
    "\n",
    "MOUTH_POINTS = list(range(48, 61))\n",
    "RIGHT_BROW_POINTS = list(range(17, 22))\n",
    "LEFT_BROW_POINTS = list(range(22, 27))\n",
    "RIGHT_EYE_POINTS = list(range(36, 42))\n",
    "LEFT_EYE_POINTS = list(range(42, 48))\n",
    "NOSE_POINTS = list(range(27, 35))\n",
    "\n",
    "POINTS = LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS + RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS\n",
    "ALIGN_POINTS = POINTS\n",
    "OVERLAY_POINTS = [POINTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeProfiler(object):\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        # logging.info(\"The %s is done in %fs\", self.label, time.time() - self.start)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam_frame(cam):\n",
    "    ret, img = cam.read()\n",
    "    img = cv2.resize(img, (640, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_landmarks(img):\n",
    "    rects = detector(img, 1)\n",
    "    print(rects)\n",
    "    if len(rects) == 0:\n",
    "        return -1\n",
    "\n",
    "    return np.matrix([[p.x, p.y] for p in predictor(img, rects[0]).parts()])\n",
    "\n",
    "def annotate_landmarks(im, landmarks):\n",
    "    im = im.copy()\n",
    "    for idx, point in enumerate(landmarks):\n",
    "        pos = (point[0, 0], point[0, 1])\n",
    "        cv2.putText(im, str(idx), pos,\n",
    "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
    "                    fontScale=0.4,\n",
    "                    color=(0, 0, 255))\n",
    "        cv2.circle(im, pos, 3, color=(0, 255, 255))\n",
    "    return im\n",
    "\n",
    "\n",
    "def draw_convex_hull(im, points, color):\n",
    "    points = cv2.convexHull(points)\n",
    "    cv2.fillConvexPoly(im, points, color=color)\n",
    "\n",
    "\n",
    "def get_face_mask(im, landmarks):\n",
    "    im = np.zeros(im.shape[:2], dtype=np.float64)\n",
    "\n",
    "    for group in OVERLAY_POINTS:\n",
    "        draw_convex_hull(im, landmarks[group], color=1)\n",
    "\n",
    "    im = np.array([im, im, im]).transpose((1, 2, 0))\n",
    "    im = cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) > 0\n",
    "    im = im * 1.0\n",
    "    im = cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "def transformation_f_points(points1, points2):\n",
    "    points1 = points1.astype(np.float64)\n",
    "    points2 = points2.astype(np.float64)\n",
    "\n",
    "    c1 = np.mean(points1, axis=0)\n",
    "    c2 = np.mean(points2, axis=0)\n",
    "\n",
    "    points1 -= c1\n",
    "    points2 -= c2\n",
    "\n",
    "    s1 = np.std(points1)\n",
    "    s2 = np.std(points2)\n",
    "\n",
    "    points1 /= s1\n",
    "    points2 /= s2\n",
    "\n",
    "    u, s, vt = np.linalg.svd(points1.T * points2)\n",
    "    r = (u * vt).T\n",
    "\n",
    "    h_stack = np.hstack(((s2 / s1) * r, c2.T - (s2 / s1) * r * c1.T))\n",
    "    return np.vstack([h_stack, np.matrix([0., 0., 1.])])\n",
    "\n",
    "\n",
    "def get_im_w_landmarks(fname):\n",
    "    im = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
    "    im = cv2.resize(im, (im.shape[1] * SCALE_FACTOR,\n",
    "                         im.shape[0] * SCALE_FACTOR))\n",
    "    s = get_landmarks(im)\n",
    "\n",
    "    return im, s\n",
    "\n",
    "\n",
    "def warp_im(im, m, dshape):\n",
    "    output_im = np.zeros(dshape, dtype=im.dtype)\n",
    "    cv2.warpAffine(im, m[:2], (dshape[1], dshape[0]), dst=output_im,\n",
    "                   borderMode=cv2.BORDER_TRANSPARENT, flags=cv2.WARP_INVERSE_MAP)\n",
    "    return output_im\n",
    "\n",
    "\n",
    "def correct_colours(im1, im2, landmarks1):\n",
    "    mean_left = np.mean(landmarks1[LEFT_EYE_POINTS], axis=0)\n",
    "    mean_right = np.mean(landmarks1[RIGHT_EYE_POINTS], axis=0)\n",
    "\n",
    "    blur_amount = COLOUR_CORRECT_BLUR * np.linalg.norm(mean_left - mean_right)\n",
    "    blur_amount = int(blur_amount)\n",
    "\n",
    "    if blur_amount % 2 == 0:\n",
    "        blur_amount += 1\n",
    "\n",
    "    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n",
    "    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n",
    "\n",
    "    # avoid division errors\n",
    "    im2_blur += (128 * (im2_blur <= 1.0)).astype(im2_blur.dtype)\n",
    "\n",
    "    return (im2.astype(np.float64) * im1_blur.astype(np.float64) /\n",
    "            im2_blur.astype(np.float64))\n",
    "\n",
    "\n",
    "def face_swap(img1, landmarks1, img2, landmarks2):\n",
    "    m = transformation_f_points(landmarks1[ALIGN_POINTS], landmarks2[ALIGN_POINTS])\n",
    "\n",
    "    mask = get_face_mask(img2, landmarks2)\n",
    "    warped_mask = warp_im(mask, m, img1.shape)\n",
    "    combined_mask = np.max([get_face_mask(img1, landmarks1), warped_mask], axis=0)\n",
    "\n",
    "    warped_img2 = warp_im(img2, m, img1.shape)\n",
    "    warped_corrected_img2 = correct_colours(img1, warped_img2, landmarks1)\n",
    "\n",
    "    return img1 * (1.0 - combined_mask) + warped_corrected_img2 * combined_mask\n",
    "\n",
    "\n",
    "def get_rotated_points(point, anchor, deg_angle):\n",
    "    angle = math.radians(deg_angle)\n",
    "    px, py = point\n",
    "    ox, oy = anchor\n",
    "\n",
    "    qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n",
    "    qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n",
    "    return [int(qx), int(qy)]\n",
    "\n",
    "\n",
    "def blend_w_transparency(face_img, overlay_image):\n",
    "    # BGR\n",
    "    overlay_img = overlay_image[:, :, :3]\n",
    "    # A\n",
    "    overlay_mask = overlay_image[:, :, 3:]\n",
    "\n",
    "    background_mask = 255 - overlay_mask\n",
    "    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)\n",
    "    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))\n",
    "    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))\n",
    "\n",
    "    # cast to 8 bit matrix\n",
    "    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))\n",
    "\n",
    "#in: camera, filter\n",
    "def glasses_filter(cam, glasses, should_show_bounds=False):\n",
    "    with TimeProfiler(\"image capture\"):\n",
    "        face = get_cam_frame(cam)\n",
    "\n",
    "    with TimeProfiler(\"face pose prediction\"):\n",
    "        landmarks = get_landmarks(face)\n",
    "\n",
    "    # glasses.shape = (height, width, rgba channels)\n",
    "    pts1 = np.float32([[0, 0], [glasses.shape[1], 0], [0, glasses.shape[0]], [glasses.shape[1], glasses.shape[0]]])\n",
    "\n",
    "    if type(landmarks) is int:\n",
    "        return\n",
    "\n",
    "    with TimeProfiler(\"transformation\"):\n",
    "        \"\"\"\n",
    "        GLASSES ANCHOR POINTS:\n",
    "\n",
    "        17 & 26 edges of left eye and right eye (left and right extrema)\n",
    "        0 & 16 edges of face across eyes (other left and right extra, interpolate between 0 & 17, 16 & 26 for half way points)\n",
    "        19 & 24 top of left and right brows (top extreme)\n",
    "        27 is centre of the eyes on the nose (centre of glasses)\n",
    "        28 is the bottom threshold of glasses (perhaps interpolate between 27 & 28 if too low) (bottom extreme)\n",
    "        \"\"\"\n",
    "\n",
    "        left_face_extreme = [landmarks[0, 0], landmarks[0, 1]]\n",
    "        right_face_extreme = [landmarks[16, 0], landmarks[16, 1]]\n",
    "        x_diff_face = right_face_extreme[0] - left_face_extreme[0]\n",
    "        y_diff_face = right_face_extreme[1] - left_face_extreme[1]\n",
    "\n",
    "        face_angle = math.degrees(math.atan2(y_diff_face, x_diff_face))\n",
    "\n",
    "        # get hypotenuse\n",
    "        face_width = math.sqrt((right_face_extreme[0] - left_face_extreme[0]) ** 2 +\n",
    "                               (right_face_extreme[1] - right_face_extreme[1]) ** 2)\n",
    "        glasses_width = face_width * 1.0\n",
    "\n",
    "        # top and bottom of left eye\n",
    "        eye_height = math.sqrt((landmarks[19, 0] - landmarks[28, 0]) ** 2 +\n",
    "                               (landmarks[19, 1] - landmarks[28, 1]) ** 2)\n",
    "        glasses_height = eye_height * 1.2\n",
    "\n",
    "        # generate bounding box from the anchor points\n",
    "        anchor_point = [landmarks[27, 0], landmarks[27, 1]]\n",
    "        tl = [int(anchor_point[0] - (glasses_width / 2)), int(anchor_point[1] - (glasses_height / 2))]\n",
    "        rot_tl = get_rotated_points(tl, anchor_point, face_angle)\n",
    "\n",
    "        tr = [int(anchor_point[0] + (glasses_width / 2)), int(anchor_point[1] - (glasses_height / 2))]\n",
    "        rot_tr = get_rotated_points(tr, anchor_point, face_angle)\n",
    "\n",
    "        bl = [int(anchor_point[0] - (glasses_width / 2)), int(anchor_point[1] + (glasses_height / 2))]\n",
    "        rot_bl = get_rotated_points(bl, anchor_point, face_angle)\n",
    "\n",
    "        br = [int(anchor_point[0] + (glasses_width / 2)), int(anchor_point[1] + (glasses_height / 2))]\n",
    "        rot_br = get_rotated_points(br, anchor_point, face_angle)\n",
    "\n",
    "        pts = np.float32([rot_tl, rot_tr, rot_bl, rot_br])\n",
    "        m = cv2.getPerspectiveTransform(pts1, pts)\n",
    "\n",
    "        rotated = cv2.warpPerspective(glasses, m, (face.shape[1], face.shape[0]))\n",
    "        result_2 = blend_w_transparency(face, rotated)\n",
    "\n",
    "        if should_show_bounds:\n",
    "            for p in pts:\n",
    "                pos = (p[0], p[1])\n",
    "                cv2.circle(result_2, pos, 2, (0, 0, 255), 2)\n",
    "                cv2.putText(result_2, str(p), pos, fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.4, color=(255, 0, 0))\n",
    "\n",
    "        cv2.imshow(\"Glasses Filter\", result_2)\n",
    "\n",
    "\n",
    "def moustache_filter(cam, moustache, should_show_bounds=False):\n",
    "    face = get_cam_frame(cam)\n",
    "    landmarks = get_landmarks(face)\n",
    "\n",
    "    # moustache.shape = (height, width, rgba channels)\n",
    "    pts1 = np.float32([[0, 0], [moustache.shape[1], 0], [0, moustache.shape[0]], [moustache.shape[1], moustache.shape[0]]])\n",
    "\n",
    "    \"\"\"\n",
    "    MOUSTACHE ANCHOR POINTS\n",
    "\n",
    "    centre anchor point is midway between 34 (top of philtrum) and 54 (bottom of philtrum)\n",
    "    width can be determined by the eyes as the mouth can move\n",
    "    height also determined by the eyes as before\n",
    "    generate as before and just modify multiplier coefficients & translate to anchor point?\n",
    "\n",
    "\n",
    "    ^^^ mouth and jaw can move, use eyes as anchor point initially then translate to philtrum position\n",
    "    \"\"\"\n",
    "\n",
    "    if type(landmarks) is not int:\n",
    "        left_face_extreme = [landmarks[0, 0], landmarks[0, 1]]\n",
    "        right_face_extreme = [landmarks[16, 0], landmarks[16, 1]]\n",
    "        x_diff_face = right_face_extreme[0] - left_face_extreme[0]\n",
    "        y_diff_face = right_face_extreme[1] - left_face_extreme[1]\n",
    "\n",
    "        face_angle = math.degrees(math.atan2(y_diff_face, x_diff_face))\n",
    "\n",
    "        # get hypotenuse\n",
    "        face_width = math.sqrt((right_face_extreme[0] - left_face_extreme[0]) ** 2 +\n",
    "                               (right_face_extreme[1] - right_face_extreme[1]) ** 2)\n",
    "        moustache_width = face_width * 0.8\n",
    "\n",
    "        # top and bottom of left eye\n",
    "        eye_height = math.sqrt((landmarks[19, 0] - landmarks[28, 0]) ** 2 +\n",
    "                               (landmarks[19, 1] - landmarks[28, 1]) ** 2)\n",
    "        glasses_height = eye_height * 0.8\n",
    "\n",
    "        # generate bounding box from the anchor points\n",
    "        brow_anchor = [landmarks[27, 0], landmarks[27, 1]]\n",
    "        tl = [int(brow_anchor[0] - (moustache_width / 2)), int(brow_anchor[1] - (glasses_height / 2))]\n",
    "        rot_tl = get_rotated_points(tl, brow_anchor, face_angle)\n",
    "\n",
    "        tr = [int(brow_anchor[0] + (moustache_width / 2)), int(brow_anchor[1] - (glasses_height / 2))]\n",
    "        rot_tr = get_rotated_points(tr, brow_anchor, face_angle)\n",
    "\n",
    "        bl = [int(brow_anchor[0] - (moustache_width / 2)), int(brow_anchor[1] + (glasses_height / 2))]\n",
    "        rot_bl = get_rotated_points(bl, brow_anchor, face_angle)\n",
    "\n",
    "        br = [int(brow_anchor[0] + (moustache_width / 2)), int(brow_anchor[1] + (glasses_height / 2))]\n",
    "        rot_br = get_rotated_points(br, brow_anchor, face_angle)\n",
    "\n",
    "        # locate new location for moustache on philtrum\n",
    "        top_philtrum_point = [landmarks[33, 0], landmarks[33, 1]]\n",
    "        bottom_philtrum_point = [landmarks[51, 0], landmarks[51, 1]]\n",
    "        philtrum_anchor = [(top_philtrum_point[0] + bottom_philtrum_point[0]) / 2,\n",
    "                           (top_philtrum_point[1] + bottom_philtrum_point[1]) / 2]\n",
    "\n",
    "        # determine distance from old origin to new origin and translate\n",
    "        anchor_distance = [int(philtrum_anchor[0] - brow_anchor[0]), int(philtrum_anchor[1] - brow_anchor[1])]\n",
    "        rot_tl[0] += anchor_distance[0]\n",
    "        rot_tl[1] += anchor_distance[1]\n",
    "        rot_tr[0] += anchor_distance[0]\n",
    "        rot_tr[1] += anchor_distance[1]\n",
    "        rot_bl[0] += anchor_distance[0]\n",
    "        rot_bl[1] += anchor_distance[1]\n",
    "        rot_br[0] += anchor_distance[0]\n",
    "        rot_br[1] += anchor_distance[1]\n",
    "\n",
    "        pts = np.float32([rot_tl, rot_tr, rot_bl, rot_br])\n",
    "        m = cv2.getPerspectiveTransform(pts1, pts)\n",
    "\n",
    "        rotated = cv2.warpPerspective(moustache, m, (face.shape[1], face.shape[0]))\n",
    "        result_2 = blend_w_transparency(face, rotated)\n",
    "\n",
    "        # annotate_landmarks(result_2, landmarks)\n",
    "\n",
    "        if should_show_bounds:\n",
    "            for p in pts:\n",
    "                pos = (p[0], p[1])\n",
    "                cv2.circle(result_2, pos, 2, (0, 0, 255), 2)\n",
    "                cv2.putText(result_2, str(p), pos, fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.4, color=(255, 0, 0))\n",
    "\n",
    "        cv2.imshow(\"Moustache Filter\", result_2)\n",
    "\n",
    "\n",
    "def face_swap_filter(cam, swap_img, swap_img_landmarks):\n",
    "    me_img = get_cam_frame(cam)\n",
    "    me_img = cv2.resize(me_img, (me_img.shape[1] * SCALE_FACTOR, me_img.shape[0] * SCALE_FACTOR))\n",
    "    me_landmarks = get_landmarks(me_img)\n",
    "\n",
    "    # me_img, me_landmarks = read_im_and_landmarks(\"resources/bryan_cranston.png\")\n",
    "\n",
    "    if type(me_landmarks) is not int:\n",
    "        m = transformation_f_points(me_landmarks[ALIGN_POINTS], swap_img_landmarks[ALIGN_POINTS])\n",
    "\n",
    "        mask = get_face_mask(swap_img, swap_img_landmarks)\n",
    "        warped_mask = warp_im(mask, m, me_img.shape)\n",
    "        combined_mask = np.max([get_face_mask(me_img, me_landmarks), warped_mask], axis=0)\n",
    "\n",
    "        warped_swap = warp_im(swap_img, m, me_img.shape)\n",
    "        warped_corrected_swap = correct_colours(me_img, warped_swap, me_landmarks)\n",
    "\n",
    "        output_im = me_img * (1.0 - combined_mask) + warped_corrected_swap * combined_mask\n",
    "        cv2.imwrite(\"swap_output.png\", output_im)\n",
    "        out = cv2.imread(\"swap_output.png\", 1)\n",
    "        cv2.imshow(\"Swap Output\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = create_capture('0')\n",
    "footage = cv2.imread('glasses.png', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangles[[(260, 201) (527, 468)]]\n",
      "rectangles[[(260, 201) (527, 468)]]\n",
      "rectangles[[(260, 201) (527, 468)]]\n",
      "rectangles[[(260, 201) (527, 468)]]\n",
      "rectangles[[(260, 201) (527, 468)]]\n",
      "rectangles[[(260, 201) (527, 468)]]\n",
      "rectangles[[(260, 201) (527, 468)]]\n",
      "rectangles[[(260, 201) (527, 468)]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        with TimeProfiler('glasses'):\n",
    "            glasses_filter(cam, footage, False)\n",
    "\n",
    "        if 0xFF & cv2.waitKey(30) == 27:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
